
# CORTEX-12: A Verifiable Visual Cortex for Grounded Perception

**CPU-only Â· Interpretable Â· Certifiable Â· Deterministic**

CORTEX-12 is a **visual representation substrate** that learns stable, interpretable 128-D embeddings from pixels using JEPA principles, contrastive alignment, and explicit memory. It prioritizes **clarity, stability, and reproducibility** over scale or benchmark performance.

> â€œWhat if we built AI that is small enough to understand, structured enough to verify, and honest enough to explain?â€

---

## ðŸ§  Core Capabilities

CORTEX-12 transforms raw pixels into **logic-ready perceptual facts**:

- âœ… **RGB â†’ 128-D latent vectors** (DINOv2 ViT-S/14 backbone + lightweight adapter)
- âœ… **Explicit semantic axes**: color, shape, size, material, orientation, location
- âœ… **Post-hoc verifiable perception** via human-readable JSON certificates
- âœ… **Fixed embedding subspaces**: e.g., â€œdimensions 64â€“79 = colorâ€
- âœ… **External, inspectable concept memory** (`memory_vector_v12.json`)
- âœ… **Compositional imagination** via structured rendering
- âœ… **CPU-only execution** â€” safe for long unattended runs (AMD Ryzen tested)

---

## ðŸŽ¯ Why CORTEX-12?

Modern AI prioritizes **scale and performance** over **trust and transparency**. CORTEX-12 offers a counter-paradigm:

| Feature | CORTEX-12 | JEPAs / LLMs / VLMs |
|--------|-----------|---------------------|
| Semantic axes certified via validation | âœ… | âŒ |
| Human-readable JSON certificates | âœ… | âŒ |
| Works without retraining | âœ… | âŒ |
| CPU-only, deterministic, safe for unattended use | âœ… | âŒ |
| Embedding subspaces = symbolic predicates | âœ… | âŒ |
| Explicit memory + JEPA principles | âœ… | âŒ |

CORTEX-12 is **not**:
- âŒ A large language model (LLM)
- âŒ A foundation model
- âŒ A generative image model
- âŒ An end-to-end task optimizer

It **is**:
- âœ… A **visual cortex module** for neuro-symbolic systems
- âœ… A **calibrated perceptual instrument**
- âœ… A research platform for **verifiable grounded perception**

---

## ðŸš€ Quick Start

```powershell
# Setup
python -m venv venv
.\venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# Run smoke test
python test_v12_smoke.py
```

> ðŸ’¡ **Requirements**: Windows 11, Python 3.10+, CPU-only PyTorch, AMD Ryzen-class CPU recommended

---

## ðŸ§ª Phase-3: Curriculum-Based Semantic Grounding (Production-Ready)

CORTEX-12 now supports **verifiable multi-attribute perception** over synthetically generated scenes with explicit control over **six grounded attributes**:

- **Color** (12 classes: red, blue, amber, chartreuse, etc.)  
- **Shape** (6 classes: square, circle, hexagon, triangle, rectangle, star)  
- **Size** (3 classes: small, medium, large)  
- **Material** (5 classes: matte, glossy, metallic, glass, fabric)  
- **Orientation** (4 views â†’ 3 certified classes due to 2D symmetry)  
- **Location** (continuous x,y coordinates)

### ðŸ”‘ Key Innovations

#### âœ… Verifiable Perception via Semantic Axis Certification
- Each attribute mapped to a **fixed subspace** of the 128-D embedding
- Runtime verification validates: *â€œdimension 64â€“79 = colorâ€*
- Human-readable **JSON certificates** replace black-box probing

#### âœ… Physically Grounded Orientation Handling
- Recognizes that **0Â° and 180Â° are visually identical** for front-facing cubes in 2D
- Merges them into a single orientation class â€” **not a bug, but a feature**
- Achieves **76.5% orientation accuracy** with **0.61 confidence**

#### âœ… Transparent Failure Modes
- Low circle confidence? â†’ **Add more circle examples**
- Amber/yellow confusion? â†’ **Refine color boundaries**
- All issues are **diagnosable and fixable** without retraining from scratch

### ðŸ“Š Performance (Final Model: `cortex_step_phase3_0200.pt`)

| Attribute | Accuracy | Avg Confidence | Status |
|----------|----------|----------------|--------|
| **Material** | 99.4% | 0.618 | âœ… Outstanding |
| **Size** | 95.6% | 0.728 | âœ… Excellent |
| **Shape** | 90.9% | 0.346 | âš ï¸ Good (circle weakness) |
| **Color** | 90.2% | 0.531 | âš ï¸ Good (amber/yellow boundary) |
| **Orientation** | 76.5% | 0.610 | âœ… Correctly handles 2D symmetry |

> ðŸ’¡ Confidence is calibrated via exponential distance-to-centroid for honest uncertainty.

### ðŸ› ï¸ Usage

```powershell
# Train (CPU-only, ~24 hours)
python train_cortex_phase3_curriculum.py --epochs 200 --batch_size 4

# Certify axes
python tools/certify_cortex12_phase3.py --checkpoint runs/phase3/cortex_step_phase3_0200.pt --output_dir certs/phase3

# Verify perception
python examples/verify_perception_phase3.py --image data/curriculum/images/red_square_medium_0deg_matte_0_25_0_25.png --checkpoint runs/phase3/cortex_step_phase3_0200.pt --cert_dir certs/phase3
```

---

## ðŸ§ª Phase-2: Tiny-ImageNet Foundation

Early training used **Tiny-ImageNet-200** to establish stable base representations:

- **Backbone**: DINOv2 ViT-S/14 (loaded via `torch.hub`)
- **Checkpoint**: `cortex_step05600.pt` (~680 KB)
- **Results**: Stable embeddings, clear concept separation, shape > size > color hierarchy

This phase validated the **JEPA-inspired architecture** before moving to controlled curriculum learning.

---

## ðŸ§© Use Cases

CORTEX-12 is ideal for applications requiring **trustworthy perception**:

- **Safety-critical robotics** (verifiable object understanding)
- **Assistive technology** (explainable visual reasoning)
- **Scientific instrumentation** (calibrated perceptual measurements)
- **Education and AI literacy** (transparent representation learning)
- **Neuro-symbolic AI** (pixels â†’ logic-ready facts)

---

## ðŸ“ Evaluation Philosophy

CORTEX-12 rejects standard accuracy benchmarks in favor of:

- **Verifiability**: Can you prove what the model knows?
- **Stability**: Do embeddings remain consistent across runs?
- **Interpretability**: Are semantic axes human-understandable?
- **Reproducibility**: Can others audit and reproduce your results?

> â€œWe measure success not by leaderboard rank, but by how much we can understand.â€

---

## ðŸ“ Key Files

### Core System
- `vl_jepa_llm_v12.py` â€” CORTEX-12 runtime (visual cortex + memory)
- `cortex_adapter_v12.py` â€” Lightweight adapter with 6 projection heads
- `brain_vector_v12.pth` â€” Active cortex weights (adapter + heads)
- `memory_vector_v12.json` â€” Explicit concept memory

### Training
- `train_cortex_phase2_tinyimagenet.py` â€” Phase-2 trainer (Tiny-ImageNet)
- `train_cortex_phase3_curriculum.py` â€” Phase-3 trainer (synthetic curriculum)

### Verification & Tools
- `tools/certify_cortex12_phase3.py` â€” Axis certification with merged labels
- `examples/verify_perception_phase3.py` â€” Runtime perception verification
- `tools/validate_labels.ps1` â€” PowerShell label validation

### Testing
- `run_all_v12_tests.py`
- `test_v12_smoke.py`
- `test_v12_compare_stability.py`
- `bench_v12_forward.py`

---

## ðŸ¤ Contributing

Contributions are welcome! Focus areas:
- Improved synthetic data generation
- Enhanced certification tooling
- New verification examples
- Documentation improvements

Please preserve the core principles: **CPU-first, verifiable, deterministic**.

---

## ðŸ“œ License

MIT License

---

## ðŸ“š Citation

If you use CORTEX-12 in research, please cite:

```bibtex
@software{cortex12,
  author = {Taylor, John},
  title = {CORTEX-12: A Verifiable Visual Cortex for Grounded Perception},
  year = {2026},
  url = {https://github.com/taylorjohn/cortex-12}
}
```

---

> **CORTEX-12 proves that you donâ€™t need scale to build systems that are simple, inspectable, and accountable.**  
> This is **perception as a calibrated scientific instrument** â€” not a black box.
```
